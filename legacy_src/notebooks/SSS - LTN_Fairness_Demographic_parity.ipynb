{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1CTy1UpyGtg"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrGrfnZIKLOl",
        "outputId": "df0e26c4-8b23-402d-fe0e-ce94b9a11f43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:\n",
            "`load_boston` has been removed from scikit-learn since version 1.2.\n",
            "\n",
            "The Boston housing prices dataset has an ethical problem: as\n",
            "investigated in [1], the authors of this dataset engineered a\n",
            "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
            "positive impact on house prices [2]. Furthermore the goal of the\n",
            "research that led to the creation of this dataset was to study the\n",
            "impact of air quality but it did not give adequate demonstration of the\n",
            "validity of this assumption.\n",
            "\n",
            "The scikit-learn maintainers therefore strongly discourage the use of\n",
            "this dataset unless the purpose of the code is to study and educate\n",
            "about ethical issues in data science and machine learning.\n",
            "\n",
            "In this special case, you can fetch the dataset from the original\n",
            "source::\n",
            "\n",
            "    import pandas as pd\n",
            "    import numpy as np\n",
            "\n",
            "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "    target = raw_df.values[1::2, 2]\n",
            "\n",
            "Alternative datasets include the California housing dataset and the\n",
            "Ames housing dataset. You can load the datasets as follows::\n",
            "\n",
            "    from sklearn.datasets import fetch_california_housing\n",
            "    housing = fetch_california_housing()\n",
            "\n",
            "for the California housing dataset and::\n",
            "\n",
            "    from sklearn.datasets import fetch_openml\n",
            "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "for the Ames housing dataset.\n",
            "\n",
            "[1] M Carlisle.\n",
            "\"Racist data destruction?\"\n",
            "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
            "\n",
            "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
            "\"Hedonic housing prices and the demand for clean air.\"\n",
            "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
            "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            ": LawSchoolGPADataset will be unavailable. To install, run:\n",
            "pip install 'aif360[LawSchoolGPA]'\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pprint import pprint\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from fairlearn.metrics import demographic_parity_ratio\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "sys.path.append(\"../\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "import tensorflow as tf\n",
        "\n",
        "from utils import StandardScaleData_ExcludingFeature, LTNOps\n",
        "import ltn\n",
        "import KnowledgeBase\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qsab7HO7wMCl"
      },
      "outputs": [],
      "source": [
        "## Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# `PYTHONHASHSEED` environment variable\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Python built-in random, numpy(+ scikit) and tensorflow seed\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# Enable TensorFlow op-determinism \n",
        "# from version 2.8 onwards https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism)\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "project = \"LTN - Demographic parity cross validation\"\n",
        "wandb_hp = dict(\n",
        "    dataset='compas',\n",
        "    sensitive_feature='sex',\n",
        "    hidden_layer_sizes=(128, 256, 128)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z_eD6JiyBAg"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxBWsJJWSgTe",
        "outputId": "13b47949-cf30-494c-c84a-531ef7ed3227"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Missing Data: 5 rows removed from CompasDataset.\n"
          ]
        }
      ],
      "source": [
        "dataset_orig = CompasDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZvCpNUxI985E"
      },
      "outputs": [],
      "source": [
        "df, attributes = dataset_orig.convert_to_dataframe()\n",
        "metadata = dataset_orig.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6z7Fand9-vN",
        "outputId": "f8167736-b73d-41fc-c676-0da1237a3eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'race': {'index': 2,\n",
            "          'maps': {0.0: 'Not Caucasian', 1.0: 'Caucasian'},\n",
            "          'name': 'race',\n",
            "          'privileged': 1.0,\n",
            "          'unprivileged': 0.0},\n",
            " 'sex': {'index': 0,\n",
            "         'maps': {0.0: 'Male', 1.0: 'Female'},\n",
            "         'name': 'sex',\n",
            "         'privileged': 1.0,\n",
            "         'unprivileged': 0.0}}\n"
          ]
        }
      ],
      "source": [
        "# Protected attributes\n",
        "\n",
        "protected_attributes = { attributes['protected_attribute_names'][k] : {\n",
        "    'name': attributes['protected_attribute_names'][k],\n",
        "    'index': attributes['feature_names'].index(attributes['protected_attribute_names'][k]),\n",
        "    'privileged': attributes['privileged_protected_attributes'][0][0],\n",
        "    'unprivileged': attributes['unprivileged_protected_attributes'][0][0],\n",
        "    'maps': metadata['protected_attribute_maps'][k]\n",
        "  }\n",
        " for k, _ in enumerate(attributes['protected_attribute_names']) }\n",
        "\n",
        "pprint(protected_attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J23A5UoTrVXO",
        "outputId": "beb47fc2-5cba-4a0c-a96d-83d4f3fdfc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata label_map: {1.0: 'Did recid.', 0.0: 'No recid.'}\n",
            "Custom label_map: {'positive': 1.0, 'negative': 0.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"Metadata label_map:\", metadata['label_maps'][0])\n",
        "label_map = {\n",
        "    \"positive\": [*metadata['label_maps'][0]][0],\n",
        "    \"negative\": [*metadata['label_maps'][0]][1]\n",
        "}\n",
        "print(\"Custom label_map:\", label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3)\n",
        "sss_splits = []\n",
        "for train_index, test_index in sss.split(dataset_orig.features, np.squeeze(dataset_orig.labels)):\n",
        "    Xtrain = dataset_orig.features[train_index]\n",
        "    Xtest = dataset_orig.features[test_index]\n",
        "    Ytrain = np.squeeze(dataset_orig.labels)[train_index]\n",
        "    Ytest = np.squeeze(dataset_orig.labels)[test_index]\n",
        "    sss_splits.append((Xtrain, Xtest, Ytrain, Ytest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainset demographic parity: 0.7306092680142173\n",
            "Testset demographic parity: 0.7414106228307491\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "\n",
        "idx = protected_attributes[wandb_hp['sensitive_feature']]['index']\n",
        "train_dp = statistics.mean([demographic_parity_ratio(Ytrain, Ytrain, sensitive_features=Xtrain[:, idx]) for Xtrain, _, Ytrain, _ in sss_splits])\n",
        "test_dp = statistics.mean([demographic_parity_ratio(Ytest, Ytest, sensitive_features=Xtest[:, idx]) for _, Xtest, _, Ytest in sss_splits])\n",
        "\n",
        "wandb_hp['trainset_demographic_parity'] = train_dp\n",
        "wandb_hp['testset_demographic_parity'] = test_dp\n",
        "print(f\"Trainset demographic parity: {wandb_hp['trainset_demographic_parity']}\")\n",
        "print(f\"Testset demographic parity: {wandb_hp['testset_demographic_parity']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ItOPcCvmxql"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split #1\n",
            "axiom_positive_class | axiom_negative_class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f421aeed430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f421aeed430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "100%|██████████| 1000/1000 [00:08<00:00, 113.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split #2\n",
            "axiom_positive_class | axiom_negative_class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 141.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split #3\n",
            "axiom_positive_class | axiom_negative_class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 139.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split #4\n",
            "axiom_positive_class | axiom_negative_class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 139.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split #5\n",
            "axiom_positive_class | axiom_negative_class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 140.13it/s]\n"
          ]
        }
      ],
      "source": [
        "ltnOps = LTNOps(ltn.fuzzy_ops.Implies_Reichenbach(), 1, 2)\n",
        "train_accuracy = 0\n",
        "train_di = 0\n",
        "test_accuracy = 0\n",
        "test_di = 0\n",
        "\n",
        "for i, (Xtrain, Xtest, Ytrain, Ytest) in enumerate(sss_splits):\n",
        "\n",
        "    print(f'Split #{i+1}')\n",
        "\n",
        "    Xtrain, Xtest, scaler = StandardScaleData_ExcludingFeature(\n",
        "        Xtrain, Xtest, protected_attributes[wandb_hp['sensitive_feature']]['index'])\n",
        "\n",
        "    kb = KnowledgeBase.KnowledgeBase(\n",
        "        Xtrain, Xtest,\n",
        "        Ytrain, Ytest,\n",
        "        label_map,\n",
        "        protected_attributes[wandb_hp['sensitive_feature']]['privileged'],\n",
        "        protected_attributes[wandb_hp['sensitive_feature']]['unprivileged'],\n",
        "        hidden_layer_sizes=wandb_hp['hidden_layer_sizes'],\n",
        "        fuzzy_ops=ltnOps,\n",
        "        sensitive_feature_index=protected_attributes[wandb_hp['sensitive_feature']]['index'],\n",
        "        config_file='./KnowledgeBaseAxioms.json'\n",
        "    )\n",
        "    \n",
        "    wandb_hp['learning_rate'] = 0.001\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=wandb_hp['learning_rate'])\n",
        "    wandb_hp['optimizer'] = optimizer.__class__\n",
        "    wandb_hp['epochs'] = 1000\n",
        "\n",
        "    wandb_init = dict(\n",
        "        project=project,\n",
        "        name=f\"{' | '.join([ax['name'] for ax in kb.config if ax['infos']['training']]) }\",\n",
        "        entity=\"albezjelt\",\n",
        "        config={\n",
        "            **wandb_hp, \n",
        "            **kb.axioms,\n",
        "            'weights': kb.weights,\n",
        "            'config.json': kb.config},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    print(wandb_init['name'])\n",
        "\n",
        "    for epoch in tqdm(range(wandb_hp['epochs'])):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = 1. - kb.train_step()  # type: ignore\n",
        "        grads = tape.gradient(loss, kb.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, kb.trainable_variables))\n",
        "\n",
        "    train_accuracy += kb.get_logs()['train_classification_metrics']['accuracy']\n",
        "    test_accuracy += kb.get_logs()['test_classification_metrics']['accuracy']\n",
        "    train_di += kb.get_logs()['fairness_metrics']['train_demographic_parity_ratio'] \n",
        "    test_di += kb.get_logs()['fairness_metrics']['test_demographic_parity_ratio']\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train acc 0.7436978683966636\n",
            "train di 0.5285093942903112\n",
            "test acc 0.6549972987574284\n",
            "test di 0.5266596732358493\n"
          ]
        }
      ],
      "source": [
        "print('train acc', train_accuracy / 5)\n",
        "print('train di', train_di / 5)\n",
        "print('test acc', test_accuracy / 5)\n",
        "print('test di', test_di / 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
